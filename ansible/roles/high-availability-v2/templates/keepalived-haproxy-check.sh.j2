#!/bin/bash
# Intelligent keepalived HAProxy Health Check Script
# Prevents cascading failures by monitoring backend health percentage
# Only triggers VIP failover when majority of backends are unhealthy
# Generated by Ansible high-availability role

set -euo pipefail

# Configuration
HAPROXY_STATS_PORT="{{ haproxy_stats_port | default('8404') }}"
HAPROXY_STATS_URI="{{ haproxy_stats_uri | default('/stats') }}"
HAPROXY_CONTAINER_NAME="jenkins-haproxy"
LOG_FILE="/var/log/keepalived-haproxy-check.log"
BACKEND_HEALTH_LOG="/var/log/keepalived-backend-health.log"
TIMEOUT=8

# Intelligent failover thresholds (prevents cascading failures)
BACKEND_HEALTH_THRESHOLD="{{ keepalived_backend_health_threshold | default(50) }}"  # Percentage of backends that must be UP
TEAM_QUORUM="{{ keepalived_team_quorum | default(2) }}"  # Minimum healthy teams to avoid failover
GRACE_PERIOD="{{ keepalived_failover_grace_period | default(30) }}"  # Seconds to wait before triggering failover
GRACE_STATE_FILE="/var/run/keepalived-grace-period.state"

# Logging functions
log_message() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') [keepalived-check] $1" | tee -a "$LOG_FILE"
}

log_backend_health() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') $1" >> "$BACKEND_HEALTH_LOG"
}

# Rotate log files if too large (>10MB)
rotate_logs() {
    for logfile in "$LOG_FILE" "$BACKEND_HEALTH_LOG"; do
        if [ -f "$logfile" ] && [ $(stat -f%%z "$logfile" 2>/dev/null || stat -c%%s "$logfile" 2>/dev/null || echo 0) -gt 10485760 ]; then
            mv "$logfile" "${logfile}.old"
            touch "$logfile"
        fi
    done
}

# Check if HAProxy container is running
check_container_running() {
    if ! {{ haproxy_container_runtime }} ps --filter "name=${HAPROXY_CONTAINER_NAME}" --filter "status=running" --format "table {% raw %}{{.Names}}{% endraw %}" | grep -q "${HAPROXY_CONTAINER_NAME}"; then
        log_message "ERROR: HAProxy container '${HAPROXY_CONTAINER_NAME}' is not running"
        return 1
    fi
    return 0
}

# Check HAProxy stats endpoint
check_haproxy_stats() {
    local stats_url="http://localhost:${HAPROXY_STATS_PORT}${HAPROXY_STATS_URI}"

    if ! curl -f -s --max-time "$TIMEOUT" "$stats_url" >/dev/null 2>&1; then
        log_message "ERROR: HAProxy stats endpoint unreachable: $stats_url"
        return 1
    fi
    return 0
}

# INTELLIGENT BACKEND HEALTH CHECKING - Prevents cascading failures
# This function analyzes per-team backend health and only triggers failover
# when the majority of backends/teams are unhealthy
check_intelligent_backend_health() {
    local stats_api_url="http://localhost:${HAPROXY_STATS_PORT}/stats;csv"

    # Get backend status CSV
    local backend_status
    if ! backend_status=$(curl -f -s --max-time "$TIMEOUT" "$stats_api_url" 2>/dev/null); then
        log_message "ERROR: Cannot retrieve HAProxy backend status from stats API"
        return 1
    fi

    # Parse backends - exclude stats, frontends, and non-Jenkins backends
    # Format: backend_name,server_name,status,...
    local total_jenkins_backends=0
    local up_jenkins_backends=0
    local team_health_status=""

    # Track per-team health (jenkins_backend_{team})
{% if jenkins_teams is defined and jenkins_teams | length > 0 %}
{% for team in jenkins_teams %}
    local {{ team.team_name }}_total=0
    local {{ team.team_name }}_up=0
{% endfor %}
{% endif %}

    # Parse CSV and count backend health
    while IFS=',' read -r backend_name server_name status rest; do
        # Skip headers, stats backend, and frontends
        if [[ "$backend_name" =~ ^jenkins_backend_ ]] && [[ "$server_name" != "BACKEND" ]]; then
            total_jenkins_backends=$((total_jenkins_backends + 1))

            # Count UP backends
            if [[ "$status" == "UP" ]]; then
                up_jenkins_backends=$((up_jenkins_backends + 1))
            fi

            # Track per-team health
{% if jenkins_teams is defined and jenkins_teams | length > 0 %}
{% for team in jenkins_teams %}
            if [[ "$backend_name" == "jenkins_backend_{{ team.team_name }}" ]]; then
                {{ team.team_name }}_total=$(( {{ team.team_name }}_total + 1 ))
                [[ "$status" == "UP" ]] && {{ team.team_name }}_up=$(( {{ team.team_name }}_up + 1 ))
            fi
{% endfor %}
{% endif %}
        fi
    done <<< "$backend_status"

    # Calculate overall backend health percentage
    local backend_health_percentage=0
    if [ "$total_jenkins_backends" -gt 0 ]; then
        backend_health_percentage=$((up_jenkins_backends * 100 / total_jenkins_backends))
    fi

    # Count healthy teams (teams with at least 1 UP backend)
    local healthy_teams=0
{% if jenkins_teams is defined and jenkins_teams | length > 0 %}
{% for team in jenkins_teams %}
    if [ "${{ team.team_name }}_up" -gt 0 ]; then
        healthy_teams=$((healthy_teams + 1))
        team_health_status="${team_health_status}{{ team.team_name }}:UP(${{ team.team_name }}_up/${{ team.team_name }}_total) "
    else
        team_health_status="${team_health_status}{{ team.team_name }}:DOWN(${{ team.team_name }}_up/${{ team.team_name }}_total) "
    fi
{% endfor %}
{% endif %}

    # Log detailed backend health status
    log_backend_health "Overall: ${up_jenkins_backends}/${total_jenkins_backends} (${backend_health_percentage}%) | Teams: ${team_health_status}"

    # INTELLIGENT FAILOVER LOGIC - Prevent cascading failures
    # Only fail if BOTH conditions are met:
    # 1. Backend health percentage below threshold (default 50%)
    # 2. Healthy teams below quorum (default 2)

    if [ "$backend_health_percentage" -lt "$BACKEND_HEALTH_THRESHOLD" ] && [ "$healthy_teams" -lt "$TEAM_QUORUM" ]; then
        # Grace period logic - don't fail immediately
        if [ -f "$GRACE_STATE_FILE" ]; then
            local grace_start
            grace_start=$(cat "$GRACE_STATE_FILE")
            local current_time
            current_time=$(date +%%s)
            local elapsed=$((current_time - grace_start))

            if [ "$elapsed" -ge "$GRACE_PERIOD" ]; then
                log_message "CRITICAL: Backend health ${backend_health_percentage}%% below threshold ${BACKEND_HEALTH_THRESHOLD}%%, healthy teams ${healthy_teams}/${TEAM_QUORUM} - TRIGGERING FAILOVER after ${elapsed}s grace period"
                rm -f "$GRACE_STATE_FILE"
                return 1
            else
                log_message "WARNING: Backend health ${backend_health_percentage}%% below threshold, waiting grace period (${elapsed}/${GRACE_PERIOD}s)"
                return 0  # Don't fail yet
            fi
        else
            # Start grace period
            date +%%s > "$GRACE_STATE_FILE"
            log_message "WARNING: Backend health ${backend_health_percentage}%% below threshold ${BACKEND_HEALTH_THRESHOLD}%%, healthy teams ${healthy_teams}/${TEAM_QUORUM} - Starting ${GRACE_PERIOD}s grace period"
            return 0  # Don't fail yet
        fi
    else
        # Health is good or single team failure - clear grace period
        [ -f "$GRACE_STATE_FILE" ] && rm -f "$GRACE_STATE_FILE"

        if [ "$backend_health_percentage" -lt "$BACKEND_HEALTH_THRESHOLD" ]; then
            log_message "INFO: Backend health ${backend_health_percentage}%% below threshold but ${healthy_teams} teams healthy - NO FAILOVER (prevents cascading failure)"
        else
            # Only log success occasionally
            if [ $(($(date +%%s) %% 60)) -eq 0 ]; then
                log_message "INFO: Backend health ${backend_health_percentage}%%, healthy teams ${healthy_teams}/${TEAM_QUORUM} - HEALTHY"
            fi
        fi
        return 0
    fi
}

# Check if HAProxy process is responsive inside container
check_haproxy_process() {
    local haproxy_pid
    if ! haproxy_pid=$({{ haproxy_container_runtime }} exec "${HAPROXY_CONTAINER_NAME}" pgrep haproxy 2>/dev/null); then
        log_message "ERROR: HAProxy process not found in container"
        return 1
    fi

    # Check if process is responsive
    if ! {{ haproxy_container_runtime }} exec "${HAPROXY_CONTAINER_NAME}" kill -0 "$haproxy_pid" 2>/dev/null; then
        log_message "ERROR: HAProxy process not responsive (PID: $haproxy_pid)"
        return 1
    fi

    return 0
}

# Main health check function
main() {
    local exit_code=0

    # Rotate logs
    rotate_logs

    # Phase 1: Check if HAProxy container and process are running
    # These are critical - if container/process is down, we fail immediately
    if ! check_container_running; then
        log_message "CRITICAL: HAProxy container not running - IMMEDIATE FAILOVER"
        exit 1
    fi

    if ! check_haproxy_process; then
        log_message "CRITICAL: HAProxy process not responsive - IMMEDIATE FAILOVER"
        exit 1
    fi

    # Phase 2: Check if HAProxy stats endpoint is accessible
    if ! check_haproxy_stats; then
        log_message "ERROR: HAProxy stats endpoint unreachable - IMMEDIATE FAILOVER"
        exit 1
    fi

    # Phase 3: INTELLIGENT backend health check (prevents cascading failures)
    if ! check_intelligent_backend_health; then
        log_message "CRITICAL: Backend health check failed threshold - TRIGGERING FAILOVER"
        exit 1
    fi

    # All checks passed
    exit 0
}

# Handle script interruption
trap 'log_message "Health check interrupted"; exit 1' INT TERM

# Execute main function
main "$@"